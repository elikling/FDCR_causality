{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "create a samll and big versions of toy example data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Simpson's Paradox\n",
        "- Create toy data where the paradox is evident\n",
        "- Fit a na√Øve regression where the relation is not intuitive\n",
        "- Then show what happens when a confounder is added (direction is changed)\n",
        "- Throwing all the variables in kind of works but specifying a causal DAG will get the estimates correctly"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, when, lit, col\n",
        "import pyspark.sql.functions as F"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1722032135213
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "\n",
        "#spark = SparkSession.builder.appName(\"ToyExamplePySpark\").getOrCreate()\n",
        "\n",
        "# Set random seed\n",
        "#spark.sparkContext.setRandomSeed(853210)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1722032135601
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the desired number of samples\n",
        "def genearate_toy_data(n_samples = 2000, out_fn = \"toy.parquet\"):\n",
        "    spark = SparkSession.builder.appName(\"ToyDataGenerator\").getOrCreate()\n",
        "    df = spark.range(n_samples)\n",
        "\n",
        "    # Generate toy data\n",
        "    df = df.withColumn(\"Winter_Ind\", F.when(F.rand() < 0.24, 1).otherwise(0))\n",
        "\n",
        "    df = df.withColumn(\"Rain_Ind\", \n",
        "                    F.when(F.rand() < (0.2 + col(\"Winter_Ind\") * 0.3), 1).otherwise(0))\n",
        "\n",
        "    df = df.withColumn(\"Speed_KMpH\", \n",
        "                    F.randn() * 0.7 + (60 - col(\"Rain_Ind\") * 0.9))\n",
        "\n",
        "    df = df.withColumn(\"Fuel_Consumption_LpKM\", \n",
        "                    F.randn() * 0.5 + (50 + col(\"Speed_KMpH\") / 4 + col(\"Rain_Ind\") * 2.1))\n",
        "\n",
        "    # Select only the columns we need\n",
        "    toy_example = df.select(\"Winter_Ind\", \"Rain_Ind\", \"Speed_KMpH\", \"Fuel_Consumption_LpKM\")\n",
        "\n",
        "    # Show the first few rows\n",
        "    toy_example.show(5)\n",
        "\n",
        "    # Save toy_example as Parquet file locally\n",
        "    toy_example.write.parquet(out_fn)\n",
        "\n",
        "    # To use the saved Parquet file later, you can read it like this:\n",
        "    # loaded_toy_example = spark.read.parquet(\"toy_example.parquet\")\n",
        "\n",
        "    # Stop the Spark session\n",
        "    spark.stop()\n",
        "\n",
        "genearate_toy_data(n_samples = 200, out_fn = \"toy_example_small.parquet\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1722032143947
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722031873949
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}