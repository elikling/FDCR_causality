{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "create a samll and big versions of toy example data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Simpson's Paradox\n",
        "- Create toy data where the paradox is evident\n",
        "- Fit a na√Øve regression where the relation is not intuitive\n",
        "- Then show what happens when a confounder is added (direction is changed)\n",
        "- Throwing all the variables in kind of works but specifying a causal DAG will get the estimates correctly"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, when, lit, col\n",
        "import pyspark.sql.functions as F"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ac476836-85ff-4d1a-95f2-f23a57b994fa",
              "statement_id": 6,
              "statement_ids": [
                6
              ],
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-07-26T21:48:38.5400527Z",
              "session_start_time": "2024-07-26T21:48:38.5809141Z",
              "execution_start_time": "2024-07-26T21:52:54.4712534Z",
              "execution_finish_time": "2024-07-26T21:52:58.0026778Z",
              "parent_msg_id": "19a08d06-6da3-4bf0-a7c0-d1c336e8088c"
            },
            "text/plain": "StatementMeta(ac476836-85ff-4d1a-95f2-f23a57b994fa, 0, 6, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1722030778231
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ToyExamplePySpark\").getOrCreate()\n",
        "# Set random seed\n",
        "#spark.sparkContext.setRandomSeed(853210)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ac476836-85ff-4d1a-95f2-f23a57b994fa",
              "statement_id": 10,
              "statement_ids": [
                10
              ],
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 0
                },
                "jobs": [],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-07-26T21:58:21.5032109Z",
              "session_start_time": null,
              "execution_start_time": "2024-07-26T21:58:21.6274605Z",
              "execution_finish_time": "2024-07-26T21:58:21.9333733Z",
              "parent_msg_id": "2653d214-e835-4d54-a10c-04146403164e"
            },
            "text/plain": "StatementMeta(ac476836-85ff-4d1a-95f2-f23a57b994fa, 0, 10, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1722031101998
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the desired number of samples\n",
        "def genearate_toy_data(n_samples = 2000, out_fn):\n",
        "df = spark.range(n_samples)\n",
        "\n",
        "# Generate toy data\n",
        "df = df.withColumn(\"Winter_Ind\", F.when(F.rand() < 0.24, 1).otherwise(0))\n",
        "\n",
        "df = df.withColumn(\"Rain_Ind\", \n",
        "                   F.when(F.rand() < (0.2 + col(\"Winter_Ind\") * 0.3), 1).otherwise(0))\n",
        "\n",
        "df = df.withColumn(\"Speed_KMpH\", \n",
        "                   F.randn() * 0.7 + (60 - col(\"Rain_Ind\") * 0.9))\n",
        "\n",
        "df = df.withColumn(\"Fuel_Consumption_LpKM\", \n",
        "                   F.randn() * 0.5 + (50 + col(\"Speed_KMpH\") / 4 + col(\"Rain_Ind\") * 2.1))\n",
        "\n",
        "# Select only the columns we need\n",
        "toy_example = df.select(\"Winter_Ind\", \"Rain_Ind\", \"Speed_KMpH\", \"Fuel_Consumption_LpKM\")\n",
        "\n",
        "# Show the first few rows\n",
        "toy_example.show(5)\n",
        "\n",
        "# Save toy_example as Parquet file locally\n",
        "toy_example.write.parquet(\"toy_example.parquet\")\n",
        "\n",
        "# To use the saved Parquet file later, you can read it like this:\n",
        "# loaded_toy_example = spark.read.parquet(\"toy_example.parquet\")\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "ac476836-85ff-4d1a-95f2-f23a57b994fa",
              "statement_id": 11,
              "statement_ids": [
                11
              ],
              "livy_statement_state": "available",
              "spark_jobs": {
                "numbers": {
                  "FAILED": 0,
                  "UNKNOWN": 0,
                  "RUNNING": 0,
                  "SUCCEEDED": 2
                },
                "jobs": [
                  {
                    "displayName": "parquet at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 43447,
                    "dataRead": 0,
                    "rowCount": 4000,
                    "usageDescription": "",
                    "jobId": 1,
                    "name": "parquet at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\n# Create a DataFrame with the desired number of samples\nn_samples = 2000\ndf = spark.range(n_samples)\n\n# Generate toy data\ndf = df.withColumn(\"Winter_Ind\", F.when(F.rand() < 0.24, 1).otherwise(0))\n\ndf = df.withColumn(\"Rain_Ind\", \n                   F.when(F.rand() < (0.2 + col(\"Winter_Ind\") * 0.3), 1).otherwise(0))\n\ndf = df.withColumn(\"Speed_KMpH\", \n                   F.randn() * 0.7 + (60 - col(\"Rain_Ind\") * 0.9))\n\ndf = df.withColumn(\"Fuel_Consumption_LpKM\", \n                   F.randn() * 0.5 + (50 + col(\"Speed_KMpH\") / 4 + col(\"Rain_Ind\") * 2.1))\n\n# Select only the columns we need\ntoy_example = df.select(\"Winter_Ind\", \"Rain_Ind\", \"Speed_KMpH\", \"Fuel_Consumption_LpKM\")\n\n# Show the first few rows\ntoy_example.show(5)\n\n# Save toy_example as Parquet file locally\ntoy_example.write.parquet(\"toy_example.parquet\")\n\n# To use the saved Parquet file later, you can read it like this:\n# loaded_toy_example = spark.read.parquet(\"toy_example.parquet\")\n\n# Stop the Spark session\nspark.stop()",
                    "submissionTime": "2024-07-26T21:58:40.711GMT",
                    "completionTime": "2024-07-26T21:58:50.669GMT",
                    "stageIds": [
                      1
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 8,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 8,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 8,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  },
                  {
                    "displayName": "showString at NativeMethodAccessorImpl.java:0",
                    "dataWritten": 0,
                    "dataRead": 0,
                    "rowCount": 6,
                    "usageDescription": "",
                    "jobId": 0,
                    "name": "showString at NativeMethodAccessorImpl.java:0",
                    "description": "Job group for statement 11:\n# Create a DataFrame with the desired number of samples\nn_samples = 2000\ndf = spark.range(n_samples)\n\n# Generate toy data\ndf = df.withColumn(\"Winter_Ind\", F.when(F.rand() < 0.24, 1).otherwise(0))\n\ndf = df.withColumn(\"Rain_Ind\", \n                   F.when(F.rand() < (0.2 + col(\"Winter_Ind\") * 0.3), 1).otherwise(0))\n\ndf = df.withColumn(\"Speed_KMpH\", \n                   F.randn() * 0.7 + (60 - col(\"Rain_Ind\") * 0.9))\n\ndf = df.withColumn(\"Fuel_Consumption_LpKM\", \n                   F.randn() * 0.5 + (50 + col(\"Speed_KMpH\") / 4 + col(\"Rain_Ind\") * 2.1))\n\n# Select only the columns we need\ntoy_example = df.select(\"Winter_Ind\", \"Rain_Ind\", \"Speed_KMpH\", \"Fuel_Consumption_LpKM\")\n\n# Show the first few rows\ntoy_example.show(5)\n\n# Save toy_example as Parquet file locally\ntoy_example.write.parquet(\"toy_example.parquet\")\n\n# To use the saved Parquet file later, you can read it like this:\n# loaded_toy_example = spark.read.parquet(\"toy_example.parquet\")\n\n# Stop the Spark session\nspark.stop()",
                    "submissionTime": "2024-07-26T21:58:33.485GMT",
                    "completionTime": "2024-07-26T21:58:39.366GMT",
                    "stageIds": [
                      0
                    ],
                    "jobGroup": "11",
                    "status": "SUCCEEDED",
                    "numTasks": 1,
                    "numActiveTasks": 0,
                    "numCompletedTasks": 1,
                    "numSkippedTasks": 0,
                    "numFailedTasks": 0,
                    "numKilledTasks": 0,
                    "numCompletedIndices": 1,
                    "numActiveStages": 0,
                    "numCompletedStages": 1,
                    "numSkippedStages": 0,
                    "numFailedStages": 0,
                    "killedTasksSummary": {}
                  }
                ],
                "limit": 20,
                "rule": "ALL_DESC"
              },
              "session_id": "0",
              "state": "finished",
              "normalized_state": "finished",
              "queued_time": "2024-07-26T21:58:28.1894671Z",
              "session_start_time": null,
              "execution_start_time": "2024-07-26T21:58:28.3097297Z",
              "execution_finish_time": "2024-07-26T21:58:56.8115972Z",
              "parent_msg_id": "02a660b3-f908-46aa-b500-33d2f9ffe8a6"
            },
            "text/plain": "StatementMeta(ac476836-85ff-4d1a-95f2-f23a57b994fa, 0, 11, Finished, Available, Finished)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "+----------+--------+------------------+---------------------+\n|Winter_Ind|Rain_Ind|        Speed_KMpH|Fuel_Consumption_LpKM|\n+----------+--------+------------------+---------------------+\n|         0|       0|59.876793912505036|    65.63448970803802|\n|         0|       0|60.386061523112275|    64.84484197561234|\n|         1|       1| 58.19972901532991|    66.01088537561259|\n|         0|       0|60.199543106452296|    65.27469509708527|\n|         1|       1| 59.65328730349418|    67.95564542203937|\n+----------+--------+------------------+---------------------+\nonly showing top 5 rows\n\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Exception in thread Thread-9 (_upload_with_retries):\nTraceback (most recent call last):\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/synapse_context_manager.py\", line 240, in _upload_with_retries\n    do_one_upload_iteration()\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/synapse_context_manager.py\", line 217, in do_one_upload_iteration\n    with open(local_path, 'rb') as local_log, open(mnt_path, 'ab') as remote_log:\nFileNotFoundError: [Errno 2] No such file or directory: '/synfs/notebook/AmlJobLogs/dcid.0fc99243-5540-4fd5-947c-3e035e1cef3c/logs/azureml/driver/stdout'\nException in thread Thread-8 (_upload_with_retries):\nTraceback (most recent call last):\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/synapse_context_manager.py\", line 240, in _upload_with_retries\n    do_one_upload_iteration()\n  File \"/home/trusted-service-user/cluster-env/env/lib/python3.10/site-packages/synapse_context_manager.py\", line 217, in do_one_upload_iteration\n    with open(local_path, 'rb') as local_log, open(mnt_path, 'ab') as remote_log:\nFileNotFoundError: [Errno 2] No such file or directory: '/synfs/notebook/AmlJobLogs/dcid.0fc99243-5540-4fd5-947c-3e035e1cef3c/logs/azureml/driver/stderr'\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1722031136881
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}