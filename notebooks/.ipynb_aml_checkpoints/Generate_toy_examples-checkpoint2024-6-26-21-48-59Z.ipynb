{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "create a samll and big versions of toy example data"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Simpson's Paradox\n",
        "- Create toy data where the paradox is evident\n",
        "- Fit a na√Øve regression where the relation is not intuitive\n",
        "- Then show what happens when a confounder is added (direction is changed)\n",
        "- Throwing all the variables in kind of works but specifying a causal DAG will get the estimates correctly"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand, when, lit, col\n",
        "import pyspark.sql.functions as F"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "statement_id": -1,
              "statement_ids": [],
              "livy_statement_state": null,
              "spark_jobs": null,
              "session_id": null,
              "state": "session_starting",
              "normalized_state": "session_starting",
              "queued_time": "2024-07-26T21:48:38.5400527Z",
              "session_start_time": "2024-07-26T21:48:38.5809141Z",
              "execution_start_time": null,
              "execution_finish_time": null,
              "parent_msg_id": "19a08d06-6da3-4bf0-a7c0-d1c336e8088c"
            },
            "text/plain": "StatementMeta(, , -1, SessionStarting, , SessionStarting)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ToyExamplePySpark\").getOrCreate()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set random seed\n",
        "spark.sparkContext.setRandomSeed(853210)\n",
        "\n",
        "# Create a DataFrame with the desired number of samples\n",
        "n_samples = 2000\n",
        "df = spark.range(n_samples)\n",
        "\n",
        "# Generate toy data\n",
        "df = df.withColumn(\"Winter_Ind\", F.when(F.rand() < 0.24, 1).otherwise(0))\n",
        "\n",
        "df = df.withColumn(\"Rain_Ind\", \n",
        "                   F.when(F.rand() < (0.2 + col(\"Winter_Ind\") * 0.3), 1).otherwise(0))\n",
        "\n",
        "df = df.withColumn(\"Speed_KMpH\", \n",
        "                   F.randn() * 0.7 + (60 - col(\"Rain_Ind\") * 0.9))\n",
        "\n",
        "df = df.withColumn(\"Fuel_Consumption_LpKM\", \n",
        "                   F.randn() * 0.5 + (50 + col(\"Speed_KMpH\") / 4 + col(\"Rain_Ind\") * 2.1))\n",
        "\n",
        "# Select only the columns we need\n",
        "toy_example = df.select(\"Winter_Ind\", \"Rain_Ind\", \"Speed_KMpH\", \"Fuel_Consumption_LpKM\")\n",
        "\n",
        "# Show the first few rows\n",
        "toy_example.show(5)\n",
        "\n",
        "# Save toy_example as Parquet file locally\n",
        "toy_example.write.parquet(\"toy_example.parquet\")\n",
        "\n",
        "# To use the saved Parquet file later, you can read it like this:\n",
        "# loaded_toy_example = spark.read.parquet(\"toy_example.parquet\")\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.8.0",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython",
      "codemirror_mode": "ipython",
      "nbconvert_exporter": "python"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}